# ExplainabilityTool

## Installing Required Libraries:

### Python and pip

Pip was used as the package manager throughout this project. 

Python Version 3.11.x

#### Recommended: Use a Virtual Environment (venv)

Follow this guide for installation of venv: https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/

With your venv active, use ```pip install -r requirements.txt ``` to install the required libraries.

## Running the Prototypes
### Prototype 1
Navigate to ```\ExplainabilityTool\FrontEnd\src\prototypeOne``` and run ```demo.py```.

### Prototyp 2
Navigate to ``` ExplainabilityTool\FrontEnd\src\prototypeTwo ``` and run ```main.py```.

## Project #99: Designing an Interactive Computing Education Tool for Teaching Machine Learning
The project brief below is from the Univeristy of Auckland's Engineering Part IV Project Portal. The page contents have been copied here in case the following link cannot be accessed in future (for whatever reason): https://part4project.foe.auckland.ac.nz/home/project/detail/4508/

### Description:
Machine Learning (ML) has become a crucial tool in solving complex problems across various domains and is highly valued by technology companies. As a result, there is a high demand for software engineers who have the skills to develop and implement ML models that can process and analyze vast amounts of data.

While the availability of ML libraries and frameworks has made it easier for software engineers to build ML models, it also requires a deep understanding of the underlying concepts and algorithms to ensure effective design and implementation. One of the major challenges in the field of ML is that its decisions and predictions are often not easily explainable. This is because many ML algorithms, such as deep learning neural networks, use complex processes to make predictions. This complexity makes it difficult for a novice to ML understand how the algorithm arrived at its conclusions and why it made certain decisions.

To address this challenge, explainability tools like LIME and SHAP have been developed to provide insights into the workings of ML models. This project aims to design and implement a computing education tool that leverages explainability to teach the key concepts of ML. The tool will be designed to work with any Python-based ML program and will rely on explainability tools such as LIME or SHAP to provide an intuitive and interactive way to learn about ML. By visualizing the workings of a ML model, students can gain a deeper understanding of how it makes predictions and solves problems, making the learning process more engaging and effective. The project will also experiment with the impact of bias or noise in data on the prediction.

 

### Outcome:
Outcomes will include desing and developing a computing education tool for teaching ML.

### Prerequisites
None

### Specialisations
Software Engineering
### Categories
AI & Machine learning
Software Development Tools and Processes 1
### Supervisor
Valerio Terragni
### Co-supervisor
Nasser Giacaman
### Team
Ahmad Barzak

Caleb Brunton
### Lab
HASEL (405.662, Lab)
